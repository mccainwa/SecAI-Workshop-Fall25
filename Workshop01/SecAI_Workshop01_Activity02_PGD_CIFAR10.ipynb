{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mccainwa/SecAI-Workshop-Fall25/blob/main/Workshop01/SecAI_Workshop01_Activity02_PGD_CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC_uvz-jNVPp"
      },
      "source": [
        "# PGD Attack and Binary Input Detector Using CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff4442VdNVPr"
      },
      "source": [
        "### **Projected Gradient Descent (PGD) Attack - Overview**\n",
        "\n",
        "\n",
        "The Projected Gradient Descent (PGD) attack is a popular method used to generate adversarial examples for neural networks, particularly for tasks like image classification. It belongs to the family of iterative attacks, meaning that instead of just perturbing the input once, it repeatedly refines the perturbation to gradually mislead the model. PGD is often considered a more robust attack than the simple Fast Gradient Sign Method (FGSM) because it involves multiple steps, allowing for more powerful adversarial examples.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "* **Adversarial Example:** A modified input that is designed to cause the model to make an incorrect prediction.\n",
        "* **Adversarial Perturbation:** A small change to the original input that makes the model misclassify it.\n",
        "* **Projected Gradient Descent (PGD):** An iterative method that modifies the input by applying gradient-based steps and then projects it back to a feasible region to ensure the perturbation is constrained.\n",
        "\n",
        "**Steps in PGD Attack**\n",
        "\n",
        "The core idea behind PGD is to iteratively update the input using the gradient of the loss function with respect to the input, ensuring that each update stays within a certain bound (usually the $ℓ_{∞}$​ norm).\n",
        "\n",
        "**1. Initialization:**\n",
        "\n",
        "PGD starts with an input $x_{0}$​ (the original image) and adds a small random noise to it, creating an initial perturbed input $x_{0}+δ_{0}​$. The perturbation is bounded by a maximum allowed norm (usually the $ℓ∞$​-norm).\n",
        "\n",
        "**2. Gradient Step:**\n",
        "\n",
        "In each iteration, the attack computes the gradient of the loss with respect to the input. The model’s loss function is usually a cross-entropy loss for classification tasks. The goal is to maximize this loss to mislead the classifier, so the gradient is computed in the direction that increases the loss.\n",
        "\n",
        "**3. Update Rule:**\n",
        "\n",
        "Each iteration updates the perturbation based on the gradient:\n",
        "$$x_{t+1}=Proj_{B_{(x0,ϵ)}}(xt+α⋅sign(∇_{x}J(xt,y)))$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $x_{t}$​ is the perturbed input at the $t-th$ iteration.\n",
        "- $α$ is the step size or learning rate for each update (determines how big the update should be).\n",
        "- $∇_{x}J(xt,y)$ is the gradient of the loss function $J$ with respect to the input $x$ (i.e., how the model's loss changes as the input changes).\n",
        "- $sign(∇_{x}J(xt,y))$ is the element-wise sign of the gradient, which ensures the perturbation moves in the direction that maximizes the loss.\n",
        "- $Proj_{B_{(x0,ϵ)}}$​ is the projection operation that ensures the perturbation stays within the allowed perturbation norm. Here, $B_{(x0,ϵ)}$ is the ball of radius $ϵ$ around the original input $x_{0}​$, typically defined using the $ℓ$∞​-norm.\n",
        "\n",
        "**4. Projection:**\n",
        "\n",
        "After each update, we project the perturbed input back into a ball of radius ϵ around the original input. This ensures that the adversarial example is not too different from the original input in terms of the allowed perturbation. The projection operator essentially clamps each pixel’s value to lie within the valid range of the image and ensures that the perturbation does not exceed the allowed ϵ-radius.\n",
        "\n",
        "**5. Repeat:**\n",
        "\n",
        "Steps 2 to 4 are repeated for a fixed number of iterations $T$, or until the attack succeeds in fooling the model.\n",
        "Formula Breakdown\n",
        "\n",
        "Let's break down the formula into its components:\n",
        "$$x_{t+1}=Proj_{B_{(x0,ϵ)}}(x_{t}+α⋅sign(∇_{x}J(x_{t},y)))$$\n",
        "\n",
        "- $sign(∇_{x}J(x_{t},y))$: This part calculates the direction of the gradient of the loss with respect to the input $x$. The sign operation ensures that the perturbation only moves in the direction of the steepest increase in the loss function.\n",
        "\n",
        "- $α⋅sign(∇_{x}J(x_{t},y))$: The step size $α$ controls how large the perturbation should be at each step. This can be adjusted to make the attack stronger or weaker.\n",
        "\n",
        "- $x_{t}+α⋅sign(∇_{x}J(x_{t},y))$: This updates the perturbed image by adding the gradient-based perturbation to the current image $x_{t}$​.\n",
        "\n",
        "- $Proj_{B_{(x0,ϵ)}}$: The projection operation ensures that the perturbation does not exceed the allowed $ℓ∞$-norm bound. The adversarial example must remain close to the original input, within a radius ϵϵ, where ϵϵ is the maximum allowed perturbation.\n",
        "    - $B(x_{0},ϵ)$ is a ball of radius $ϵ$ centered at $x_{0}$​ in the input space.\n",
        "    - This projection typically clips the perturbed image values so that they remain within the valid range of pixel values (e.g., [0, 255] for image pixels).\n",
        "\n",
        "**How PGD Works During the Attack**\n",
        "\n",
        "1. **Start:** Initialize the perturbed image $x_{0}+δ_{0}$​, where $δ_{0}$​ is a small random perturbation added to the original input $x_{0}$.\n",
        "\n",
        "2. **Iterative Steps:**\n",
        "    - For each iteration, compute the gradient of the loss function with respect to the current perturbed image $x_{t}$​, i.e., $∇_{x}J(x_{t},y)$.\n",
        "    - Update the perturbed image $x_{t}$​ by adding the gradient’s sign, scaled by a step size $α$.\n",
        "    - Project the updated image back into the $ϵ$-ball around the original image to ensure the perturbation does not exceed the allowable bound.\n",
        "\n",
        "3. **End:** After a fixed number of iterations or when the attack succeeds in fooling the classifier, stop. The final perturbed image $x_{T}$​ is the adversarial example.\n",
        "\n",
        "**Strengths of PGD**\n",
        "\n",
        "* **Stronger Attack:** Since PGD uses multiple iterations, it can generate stronger adversarial examples compared to single-step methods like FGSM.\n",
        "* **Versatility:** PGD works with various types of loss functions, making it applicable to a wide range of tasks and models.\n",
        "* **Robustness:** It is often more successful in fooling models, as the iterative refinement allows the attack to find more subtle and effective perturbations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNaD5xqrNVPt"
      },
      "source": [
        "## **Implementation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3DabCBGNVPt"
      },
      "source": [
        "This notebook shows the PGD attack implmentation using the ART library against deep learning models trained on CIFAR-10 dataset. The notebook covers the following:\n",
        "\n",
        "* Load the required libariries\n",
        "* Load the dataset and inspect the data\n",
        "* Create a deep learning model\n",
        "* Train and evaluate the deep learning model on CIFAR test data\n",
        "* Implement PGD attack using ART\n",
        "* Create adversarial samples using the PDG attack\n",
        "* Evaluate the deep leaarning model against the adversaarial samples\n",
        "* Create a detector model to detect adversarial samples\n",
        "* Train and evaluate the detector model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqVeLlD4NVPu"
      },
      "source": [
        "### Install required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PcOL9RIH6EZU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9164fb27-b2e5-484f-e0a2-9cd638e38267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting adversarial-robustness-toolbox\n",
            "  Downloading adversarial_robustness_toolbox-1.20.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (1.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (75.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (3.6.0)\n",
            "Downloading adversarial_robustness_toolbox-1.20.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: adversarial-robustness-toolbox\n",
            "Successfully installed adversarial-robustness-toolbox-1.20.1\n",
            "Collecting visualkeras\n",
            "  Downloading visualkeras-0.1.4-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from visualkeras) (11.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.12/dist-packages (from visualkeras) (2.0.2)\n",
            "Collecting aggdraw>=1.3.11 (from visualkeras)\n",
            "  Downloading aggdraw-1.3.19-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (655 bytes)\n",
            "Downloading visualkeras-0.1.4-py3-none-any.whl (17 kB)\n",
            "Downloading aggdraw-1.3.19-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: aggdraw, visualkeras\n",
            "Successfully installed aggdraw-1.3.19 visualkeras-0.1.4\n"
          ]
        }
      ],
      "source": [
        "# Install the Adversarial Robustness Toolbox for creating and evaluating adversarial attacks.\n",
        "!pip install adversarial-robustness-toolbox\n",
        "\n",
        "# Install VisualKeras for visualizing model architecture with layer types and connections.\n",
        "!pip install visualkeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaA2YOjN6QZs"
      },
      "outputs": [],
      "source": [
        "# Suppress warnings to keep the output clean.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import TensorFlow and other necessary libraries.\n",
        "import tensorflow as tf\n",
        "# (Optional) Disable eager execution for TensorFlow v1 compatibility.\n",
        "# tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "\n",
        "# Import essential Keras components for building a CNN model.\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D\n",
        "from keras.layers import Dropout, Flatten, BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "\n",
        "# Import tools from Adversarial Robustness Toolbox (ART).\n",
        "from art.utils import load_cifar10, preprocess, to_categorical  # For loading and preparing the CIFAR-10 dataset.\n",
        "\n",
        "# Import ART components for model wrapping, attacks, and defenses.\n",
        "from art.estimators.classification import KerasClassifier          # Wraps a Keras model for ART compatibility.\n",
        "from art.attacks.evasion import ProjectedGradientDescent           # Defines an adversarial attack method.\n",
        "from art.defences.detector.evasion import BinaryInputDetector      # Used for detecting adversarial examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiUWkGWM7Mn2"
      },
      "source": [
        "### Load CIFAR-10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpw_jCNy7Sp9"
      },
      "outputs": [],
      "source": [
        "# Load the CIFAR-10 dataset using ART's utility function.\n",
        "# This function returns pre-split training and testing data along with their min/max pixel values.\n",
        "(x_train, y_train), (x_test, y_test), min_, max_ = load_cifar10()\n",
        "\n",
        "# Define the class names corresponding to CIFAR-10 labels for reference.\n",
        "class_name = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "              'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Retrieve the minimum and maximum pixel values from the training data\n",
        "# (should typically be in the range [0, 255] before normalization).\n",
        "min_val = x_train.min()\n",
        "max_val = x_train.max()\n",
        "\n",
        "# Output dataset shapes and value range for verification.\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape, min_val, max_val\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vkmOb8R7hLh"
      },
      "source": [
        "### Inspect the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hy2vKXLV7jb0"
      },
      "outputs": [],
      "source": [
        "# Plot 10 sample images from the CIFAR-10 training set with their corresponding class labels.\n",
        "fig, ax = plt.subplots(2, 5, figsize=(15, 7))  # Create a 2x5 grid of subplots.\n",
        "ax = ax.flatten()  # Flatten the axes array for easy iteration.\n",
        "\n",
        "# Display the first 10 training images along with their human-readable class names.\n",
        "for i, (image, label) in enumerate(zip(x_train[:10], y_train[:10])):\n",
        "    ax[i].imshow(image)  # Show the image (in RGB).\n",
        "\n",
        "    # Decode the one-hot encoded label back to its class index, then get the class name.\n",
        "    ax[i].set_title(f\"Label: {class_name[label.argmax()]}\", fontsize=10)\n",
        "    ax[i].axis('off')  # Hide axis ticks for cleaner visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqaFFAK1cdv5"
      },
      "source": [
        "## **Training the Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d21sTwhEcfUD"
      },
      "source": [
        "### **Model Description (CIFAR-10 CNN)**\n",
        "\n",
        "This convolutional neural network (CNN) is designed for image classification on the CIFAR-10 dataset. The architecture includes multiple convolutional blocks, each followed by regularization techniques such as dropout and batch normalization to improve performance and reduce overfitting.\n",
        "\n",
        "* **Input Layer**: Accepts RGB images of shape (32, 32, 3).\n",
        "\n",
        "* **Convolutional Blocks**:\n",
        "\n",
        "  * **Block 1**: Two convolutional layers with 32 filters each, followed by max pooling, dropout (25%), and batch normalization.\n",
        "  * **Block 2**: Two convolutional layers with 64 filters, followed by max pooling, dropout (25%), and batch normalization.\n",
        "  * **Block 3**: Two convolutional layers with 128 filters, followed by max pooling, dropout (25%), and batch normalization.\n",
        "  * **Block 4**: Two convolutional layers with 256 filters, followed by max pooling, dropout (25%), and batch normalization.\n",
        "\n",
        "* **Fully Connected Layers**:\n",
        "\n",
        "  * Dense layer with 512 units (ReLU activation) and dropout (50%).\n",
        "  * Dense layer with 64 units (ReLU activation) and dropout (50%).\n",
        "\n",
        "* **Output Layer**: Dense layer with 10 units and softmax activation to classify the input image into one of 10 CIFAR-10 classes.\n",
        "\n",
        "The model is compiled using the **Adam optimizer**, with **categorical cross-entropy** as the loss function and **accuracy** as the evaluation metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BhOVc_JNVPz"
      },
      "outputs": [],
      "source": [
        "# Define a deeper Convolutional Neural Network (CNN) for CIFAR-10 image classification.\n",
        "model = tf.keras.models.Sequential([\n",
        "    # First convolutional block: two Conv2D layers followed by pooling, dropout, and batch normalization\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "    # Second convolutional block\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "    # Third convolutional block\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "    # Fourth convolutional block\n",
        "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "    # Flatten feature maps to feed into dense layers\n",
        "    tf.keras.layers.Flatten(),\n",
        "\n",
        "    # Fully connected layers with dropout for regularization\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "\n",
        "    # Output layer with 10 units (one per class) and softmax activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer, categorical crossentropy loss, and accuracy metric\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8M2LJNANVPz"
      },
      "source": [
        "### Visualize the CNN Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXNa93ObNVP0"
      },
      "outputs": [],
      "source": [
        "import visualkeras\n",
        "# Display a layered visual representation of the model architecture.\n",
        "# 'scale_xy' controls the scaling of the diagram.\n",
        "# 'legend=True' shows layer types with corresponding color codes.\n",
        "visualkeras.layered_view(model, scale_xy=10, legend=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WlIwubuNVP0"
      },
      "source": [
        "### Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohWXwBK8NVP0"
      },
      "outputs": [],
      "source": [
        "# Print a detailed summary of the model architecture,\n",
        "# including layer types, output shapes, and number of parameters.\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5i1iMo0NVP1"
      },
      "source": [
        "### Visualize Model Architecture Diagram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Jk3tJQnNVP1"
      },
      "outputs": [],
      "source": [
        "# Generate and display a plot of the model architecture,\n",
        "# showing each layer’s name and output shape.\n",
        "# tf.keras.utils.plot_model(model, show_shapes=True, dpi = 75)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDMC9kPQNVP1"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szJbFfgONVP1"
      },
      "outputs": [],
      "source": [
        "# Train the model on the training data for 250 epochs.\n",
        "# The history object stores training metrics and loss values.\n",
        "# history = model.fit(x_train, y_train, batch_size=128, epochs = 250)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj0YvT1NNVP2"
      },
      "source": [
        "### Save/load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGdqfsggNVP2"
      },
      "outputs": [],
      "source": [
        "# If you have trained your own model and want to save it, uncomment and run:\n",
        "model_path = './cifar10_model.keras'\n",
        "# model.save(model_path)\n",
        "\n",
        "# Otherwise, load the pretrained model provided.\n",
        "# Pretrained model raw link from GitHub\n",
        "url = 'https://raw.githubusercontent.com/mnazari123/SecAI-Workshop-Fall25/main/Models/General_Models/cifar10_model.keras'\n",
        "\n",
        "# Download the model only if it doesn't exist\n",
        "if not os.path.exists(model_path):\n",
        "    os.system(f\"wget {url} -O {model_path}\")\n",
        "\n",
        "# Load the pretrained model\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "# Display model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9nqwfmY7xyr"
      },
      "source": [
        "## Evaluate model accuracy on test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gi7C4H6zg_Px"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test dataset and get loss and accuracy.\n",
        "loss_test, accuracy_test = model.evaluate(x_test, y_test)\n",
        "\n",
        "# Print the test accuracy as a percentage.\n",
        "print('Accuracy on test data: {:4.2f}%'.format(accuracy_test * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xzuNuVl73AK"
      },
      "source": [
        "### Wrap the Keras Model with an ART Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4yNT_oN72mf"
      },
      "outputs": [],
      "source": [
        "# Create an ART KerasClassifier to enable adversarial robustness tools.\n",
        "# 'clip_values' defines the input data range for normalization and attack generation.\n",
        "classifier = KerasClassifier(model=model, clip_values=(min_val, max_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU7gkxLTNVP4"
      },
      "source": [
        "## **Projected Gradient Descent (PGD)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRVEGgXt8FgT"
      },
      "source": [
        "### Overview\n",
        "\n",
        "> class art.attacks.evasion.ProjectedGradientDescent(estimator: CLASSIFIER_LOSS_GRADIENTS_TYPE | OBJECT_DETECTOR_TYPE, norm: int | float | str = inf, eps: int | float | ndarray = 0.3, eps_step: int | float | ndarray = 0.1, decay: float | None = None, max_iter: int = 100, targeted: bool = False, num_random_init: int = 0, batch_size: int = 32, random_eps: bool = False, summary_writer: str | bool | SummaryWriter = False, verbose: bool = True)\n",
        "\n",
        "The Projected Gradient Descent attack is an iterative method in which, after each iteration, the perturbation is projected on an lp-ball of specified radius (in addition to clipping the values of the adversarial sample so that it lies in the permitted data range). This is the attack proposed by Madry et al. for adversarial training.\n",
        "* Paper link: https://arxiv.org/abs/1706.06083\n",
        "\n",
        "*\\_\\_init__(estimator: CLASSIFIER_LOSS_GRADIENTS_TYPE | OBJECT_DETECTOR_TYPE, norm: int | float | str = inf, eps: int | float | ndarray = 0.3, eps_step: int | float | ndarray = 0.1, decay: float | None = None, max_iter: int = 100, targeted: bool = False, num_random_init: int = 0, batch_size: int = 32, random_eps: bool = False, summary_writer: str | bool | SummaryWriter = False, verbose: bool = True)*\n",
        "\n",
        "**Create a ProjectedGradientDescent instance.**\n",
        "\n",
        "    Parameters:\n",
        "* estimator – An trained estimator.\n",
        "* norm – The norm of the adversarial perturbation supporting “inf”, np.inf, 1 or 2.\n",
        "* eps – Maximum perturbation that the attacker can introduce.\n",
        "* eps_step – Attack step size (input variation) at each iteration.\n",
        "* random_eps (bool) – When True, epsilon is drawn randomly from truncated normal distribution. The literature suggests this for FGSM based training to generalize across different epsilons. eps_step is modified to preserve the ratio of eps / eps_step. The effectiveness of this method with PGD is untested (https://arxiv.org/pdf/1611.01236.pdf).\n",
        "* decay – Decay factor for accumulating the velocity vector when using momentum.\n",
        "* max_iter (int) – The maximum number of iterations.\n",
        "* targeted (bool) – Indicates whether the attack is targeted (True) or untargeted (False).\n",
        "* num_random_init (int) – Number of random initialisations within the epsilon ball. For num_random_init=0 starting at the original input.\n",
        "* batch_size (int) – Size of the batch on which adversarial samples are generated.\n",
        "* summary_writer – Activate summary writer for TensorBoard. Default is False and deactivated summary writer. If True save runs/CURRENT_DATETIME_HOSTNAME in current directory. If of type str save in path. If of type SummaryWriter apply provided custom summary writer. Use hierarchical folder structure to compare between runs easily. e.g. pass in ‘runs/exp1’, ‘runs/exp2’, etc. for each new experiment to compare across them.\n",
        "* verbose (bool) – Show progress bars.\n",
        "\n",
        "    \n",
        "**generate(x: ndarray, y: ndarray | None = None, \\*\\*kwargs) → ndarray**\n",
        "Generate adversarial samples and return them in an array.\n",
        "    \n",
        "    \n",
        "    Return type:\n",
        "* ndarray\n",
        "    \n",
        "    \n",
        "    Parameters:\n",
        "* x (ndarray) – An array with the original inputs.\n",
        "* y – Target values (class labels) one-hot-encoded of shape (nb_samples, nb_classes) or indices of shape (nb_samples,). Only provide this parameter if you’d like to use true labels when crafting adversarial samples. Otherwise, model predictions are used as labels to avoid the “label leaking” effect (explained in this paper: https://arxiv.org/abs/1611.01236). Default is None.\n",
        "* mask (np.ndarray) – An array with a mask broadcastable to input x defining where to apply adversarial perturbations. Shape needs to be broadcastable to the shape of x and can also be of the same shape as x. Any features for which the mask is zero will not be adversarially perturbed.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "* An array holding the adversarial examples.\n",
        "\n",
        "    \n",
        "**set_params(\\*\\*kwargs) → None**\n",
        "\n",
        "Take in a dictionary of parameters and apply attack-specific checks before saving them as attributes.\n",
        "\n",
        "    Parameters:\n",
        "* kwargs – A dictionary of attack-specific parameters.\n",
        "\n",
        "    \n",
        "    property summary_writer\n",
        "* The summary writer.\n",
        "\n",
        "Link: https://adversarial-robustness-toolbox.readthedocs.io/en/latest/modules/attacks/evasion.html#projected-gradient-descent-pgd\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXPE0oYG8Lqk"
      },
      "source": [
        "### Create a ART Projected Gradient Descent attack."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8mRmKoY8cfU"
      },
      "outputs": [],
      "source": [
        "# eps – defines the attack step size (input variation). The smaller the eps, the little the attack scale would be.\n",
        "attack_pgd = ProjectedGradientDescent(estimator=classifier, eps=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_--F4YGL92nf"
      },
      "source": [
        "### Generate adversarial test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfugBv9H9s4x"
      },
      "outputs": [],
      "source": [
        "# Reduce test set size to 500 samples for faster adversarial generation and evaluation.\n",
        "x_test = x_test[:500]\n",
        "y_test = y_test[:500]\n",
        "\n",
        "# Generate adversarial examples from the reduced test set using the PGD attack.\n",
        "x_test_adv = attack_pgd.generate(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3hXErZ_NVP5"
      },
      "source": [
        "### Save or Load PGD-Generated Adversarial Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0qgEfikNVP5"
      },
      "outputs": [],
      "source": [
        "# Option 1: Save your newly generated adversarial test data (uncomment to use).\n",
        "pgd_data_path = 'pgd_poison_data_cifar10_test_S500.npz'\n",
        "# np.savez(pgd_data_path,\n",
        "#          x_test_adv=x_test_adv, y_test_adv=y_test)\n",
        "\n",
        "# Option 2: Load previously saved adversarial test data (recommended for reuse or quick experiments).\n",
        "# Download the data file from GitHub raw URL\n",
        "poison_data_url = 'https://github.com/mnazari123/SecAI-Workshop-Fall25/raw/main/Datasets/SecAI_Workshop01_Datasets/SecAI_Workshop01_PGD_datasests/pgd_poison_data_cifar10_test_S500.npz'\n",
        "\n",
        "# Download the model only if it doesn't exist\n",
        "if not os.path.exists(pgd_data_path):\n",
        "    os.system(f\"wget {poison_data_url} -O {pgd_data_path}\")\n",
        "\n",
        "x_test_data = np.load(pgd_data_path)\n",
        "x_test_adv, y_test = x_test_data['x_test_adv'], x_test_data['y_test_adv']\n",
        "x_test_adv.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZWR3YD498H9"
      },
      "source": [
        "### Visualise the first adversarial test sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-W-_o2WJ983J"
      },
      "outputs": [],
      "source": [
        "# Display the first 10 adversarial examples along with their actual and predicted labels.\n",
        "fig, ax = plt.subplots(2, 5, figsize=(15, 7))\n",
        "ax = ax.flatten()\n",
        "\n",
        "for i, (image, label) in enumerate(zip(x_test_adv[:10], y_test[:10])):\n",
        "    # Display the adversarial image.\n",
        "    ax[i].imshow(image)\n",
        "\n",
        "    # Show actual and predicted labels.\n",
        "    prediction = model.predict(np.expand_dims(image, axis=0)).argmax()\n",
        "    ax[i].set_title(f\"Actual: {class_name[label.argmax()]} \\nPredicted: {class_name[prediction]}\", fontsize=12)\n",
        "    ax[i].axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2Wfw30YA24e"
      },
      "source": [
        "\n",
        "### Evaluate the Model on Adversarial Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yDNrRRF8f3h"
      },
      "outputs": [],
      "source": [
        "# Evaluate model performance on adversarial examples.\n",
        "loss_test, accuracy_test = model.evaluate(x_test_adv, y_test)\n",
        "\n",
        "# Calculate the average perturbation introduced by the adversarial attack.\n",
        "perturbation = np.mean(np.abs(x_test_adv - x_test))\n",
        "\n",
        "# Print evaluation results.\n",
        "print('Accuracy on adversarial test data: {:4.2f}%'.format(accuracy_test * 100))\n",
        "print('Average perturbation: {:4.2f}'.format(perturbation))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVhsjtfZWuXw"
      },
      "source": [
        "## Prepare Training Data for Adversarial Sample Detector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYISeNA8NVP6"
      },
      "outputs": [],
      "source": [
        "# Generate new adversarial examples from a subset of the training data using PGD.\n",
        "x_train_adv = attack_pgd.generate(x_train[:500])\n",
        "nb_train = x_train[:500].shape[0]\n",
        "\n",
        "# Combine clean and adversarial samples to create a binary classification dataset for detection.\n",
        "x_train_detector = np.concatenate((x_train[:500], x_train_adv), axis=0)\n",
        "\n",
        "# Create labels: [1, 0] for clean samples, [0, 1] for adversarial samples.\n",
        "y_train_detector = np.concatenate((\n",
        "    np.array([[1, 0]] * nb_train),  # Clean\n",
        "    np.array([[0, 1]] * nb_train)   # Adversarial\n",
        "), axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBGqm77HNVP7"
      },
      "source": [
        "### Save or Load Adversarial Detector Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxGi4s8nW6vY"
      },
      "outputs": [],
      "source": [
        "# Option 1: Save generated clean and adversarial samples for training the detector.\n",
        "pgd_train_data_path = \"pgd_poison_data_cifar10_train_S1000.npz\"\n",
        "# np.savez(pgd_train_data_path,\n",
        "#          x_train_detector=x_train_detector, y_train_detector=y_train_detector)\n",
        "\n",
        "# Option 2: Load pre-saved detector training data from GitHub\n",
        "url = \"https://github.com/mnazari123/SecAI-Workshop-Fall25/raw/main/Datasets/SecAI_Workshop01_Datasets/SecAI_Workshop01_PGD_datasests/pgd_poison_data_cifar10_train_S1000.npz\"\n",
        "\n",
        "# Download once if not already present\n",
        "if not os.path.exists(pgd_train_data_path):\n",
        "    os.system(f\"wget {url} -O {pgd_train_data_path}\")\n",
        "\n",
        "# Load the .npz dataset\n",
        "detector_data = np.load(pgd_train_data_path, allow_pickle=True)\n",
        "x_train_detector, y_train_detector = detector_data['x_train_detector'], detector_data['y_train_detector']\n",
        "\n",
        "# Verify the shape of loaded data\n",
        "print(x_train_detector.shape, y_train_detector.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T0Ramf5NVP7"
      },
      "source": [
        "## Build a similar CNN architecture model for detecting Adversarial Inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TkwyQ0WNVP7"
      },
      "outputs": [],
      "source": [
        "detector_model = tf.keras.models.Sequential([\n",
        "    # First convolutional block: two Conv2D layers followed by pooling, dropout, and batch normalization\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "    # Second convolutional block\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "    # Third convolutional block\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "    # Fourth convolutional block\n",
        "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "    # Flatten feature maps to feed into dense layers\n",
        "    tf.keras.layers.Flatten(),\n",
        "\n",
        "    # Fully connected layers with dropout for regularization\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "\n",
        "    # Output layer with 2 units (one per class) and softmax activation\n",
        "    tf.keras.layers.Dense(2, activation='softmax') # Two output classes: [clean, adversarial]\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer, categorical crossentropy loss, and accuracy metric\n",
        "detector_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1THkf25YNVP7"
      },
      "source": [
        "### Train the Adversarial Detector Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wg6Wn0NCb2hw"
      },
      "outputs": [],
      "source": [
        "# Train the adversarial detector model for 10 epochs.\n",
        "# Since the goal is to distinguish between clean and adversarial inputs,\n",
        "# a few epochs (e.g., 10) are typically sufficient to learn this binary classification task.\n",
        "history = detector_model.fit(x_train_detector, y_train_detector, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYxSWDomNVP8"
      },
      "outputs": [],
      "source": [
        "# Define path to save/load the detector model\n",
        "detector_model_path = './pgd_detector_model_cifar10.keras'\n",
        "detector_model.save(detector_model_path)\n",
        "'''\n",
        "# Load the pre-trained detector model from the specified path\n",
        "# Pretrained model raw link from GitHub\n",
        "url = ''\n",
        "\n",
        "# Download the model only if it doesn't exist\n",
        "if not os.path.exists(detector_model_path):\n",
        "    os.system(f\"wget {url} -O {detector_model_path}\")\n",
        "\n",
        "# Load the pretrained model\n",
        "detector = tf.keras.models.load_model(detector_model_path)\n",
        "\n",
        "# Display model summary\n",
        "detector.summary()\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EggTY2lZNVP9"
      },
      "source": [
        "## Prepare test samples for the detector model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gr5D-ciAbpYe"
      },
      "outputs": [],
      "source": [
        "# Get the number of original test samples\n",
        "nb_test = x_test.shape[0]\n",
        "\n",
        "# Combine clean and adversarial test images into one array\n",
        "x_test_detector = np.concatenate((x_test, x_test_adv), axis=0)\n",
        "\n",
        "# Create corresponding labels: [1, 0] for clean, [0, 1] for adversarial\n",
        "y_test_detector = np.concatenate((np.array([[1, 0]] * nb_test),\n",
        "                                  np.array([[0, 1]] * nb_test)), axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluat the result\n",
        "\n"
      ],
      "metadata": {
        "id": "gMZMPRGmg_iO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_test, accuracy_test = detector_model.evaluate(x_test_detector, y_test_detector)\n",
        "print('Accuracy on test data: {:4.2f}%'.format(accuracy_test * 100))"
      ],
      "metadata": {
        "id": "h94C56SahULF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIIo3t-ZNVP9"
      },
      "source": [
        "### Wrap detector model for adversarial detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmjECB4ldInD"
      },
      "outputs": [],
      "source": [
        "# Wrap the Keras detector model with ART's KerasClassifier\n",
        "detector_classifier = KerasClassifier(clip_values=(0.0, 0.1), model=detector_model, use_logits=False)\n",
        "\n",
        "# Create a BinaryInputDetector using the wrapped classifier\n",
        "detector = BinaryInputDetector(detector_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFnfhk2XNVP9"
      },
      "source": [
        "## Train the BinaryInputDetector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FV8bR3WMdUo8"
      },
      "outputs": [],
      "source": [
        "# Train the detector on labeled clean and adversarial training data\n",
        "detector.fit(x_train_detector, y_train_detector, nb_epochs=10, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMBnkB9nekcq"
      },
      "source": [
        "#### Evaluating the detector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoprCr8beek3"
      },
      "outputs": [],
      "source": [
        "# Apply the detector to the adversarial test data\n",
        "_, is_adversarial = detector.detect(x_test_adv)\n",
        "\n",
        "# Count how many adversarial samples were flagged\n",
        "flag_adv = np.sum(is_adversarial)\n",
        "\n",
        "# Display detection results\n",
        "print(\"Adversarial test data (first 500 images):\")\n",
        "print(\"Flagged: {}\".format(flag_adv))\n",
        "print(\"Not flagged: {}\".format(500 - flag_adv))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze_R17W0el9D"
      },
      "source": [
        "#### Evaluating the detector on clean (non-adversarial) data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl90T7vqegwG"
      },
      "outputs": [],
      "source": [
        "# Apply the detector to the first 500 clean test samples\n",
        "_, is_adversarial = detector.detect(x_test[:500])\n",
        "\n",
        "# Count how many clean samples were incorrectly flagged as adversarial\n",
        "flag_original = np.sum(is_adversarial)\n",
        "\n",
        "# Display detection results\n",
        "print(\"Original test data (first 500 images):\")\n",
        "print(\"Flagged: {}\".format(flag_original))\n",
        "print(\"Not flagged: {}\".format(500 - flag_original))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5svQQgaeo9d"
      },
      "source": [
        "#### Evaluate the detector across different attack strengths (eps values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfMAL97Seqgf"
      },
      "outputs": [],
      "source": [
        "# Note: Detector was trained with adversarial samples generated using eps = 0.2\n",
        "\n",
        "eps_range = [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "nb_flag_adv = []      # Number of adversarial examples flagged by the detector\n",
        "nb_missclass = []     # Number of adversarial examples that fooled the classifier\n",
        "\n",
        "for eps in eps_range:\n",
        "    # Update PGD attack with current epsilon\n",
        "    attack_pgd.set_params(**{'eps': eps})\n",
        "\n",
        "    # Generate adversarial examples for current eps\n",
        "    x_test_adv = attack_pgd.generate(x_test[:100])\n",
        "\n",
        "    # Count how many were flagged by the detector\n",
        "    nb_flag_adv.append(np.sum(detector.detect(x_test_adv)[1]))\n",
        "\n",
        "    # Count how many were misclassified by the classifier\n",
        "    nb_missclass.append(\n",
        "        np.sum(np.argmax(classifier.predict(x_test_adv), axis=1) != np.argmax(y_test[:100], axis=1))\n",
        "    )\n",
        "\n",
        "# Add clean data evaluation (eps=0) to the results\n",
        "eps_range = [0] + eps_range\n",
        "nb_flag_adv = [flag_original] + nb_flag_adv  # flag_original from clean data detection\n",
        "nb_missclass = [2] + nb_missclass            # 2 misclassifications in clean test subset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA06IWmBNVQG"
      },
      "source": [
        "#### Plot detector performance vs. classifier errors for varying attack strengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kz8X5H95fNfJ"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Plot number of adversarial samples flagged by the detector\n",
        "ax.plot(np.array(eps_range)[:10], np.array(nb_flag_adv)[:10], 'b--', label='Detector flags')\n",
        "\n",
        "# Plot number of adversarial samples misclassified by the classifier\n",
        "ax.plot(np.array(eps_range)[:10], np.array(nb_missclass)[:10], 'r--', label='Classifier errors')\n",
        "\n",
        "# Customize legend\n",
        "legend = ax.legend(loc='center right', shadow=True, fontsize='large')\n",
        "legend.get_frame().set_facecolor('#00FFCC')\n",
        "\n",
        "# Add axis labels\n",
        "plt.xlabel('Attack strength (eps)')\n",
        "plt.ylabel('Per 100 adversarial samples')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s-ZP9HSB0R5"
      },
      "source": [
        "## **Summary of PGD Attack Implementation on CIFAR10**\n",
        "\n",
        "\n",
        "#### **Overview**\n",
        "\n",
        "I have implemented the Projected Gradient Descent (PGD) attack on the CIFAR-10 dataset, which contains 60,000 color images in 10 different classes. The PGD attack is a prominent adversarial attack technique used to evaluate the robustness of machine learning models, especially deep neural networks, against adversarial perturbations. This summary provides an explanation of the PGD attack, details its implementation, and discusses the impact of the perturbation magnitude (epsilon, $ε$) on the attack's effectiveness.\n",
        "\n",
        "#### **PGD Attack Mechanism**\n",
        "\n",
        "The PGD attack is an iterative method that generates adversarial examples by applying small perturbations to the input data. The goal is to make these perturbed inputs cause the model to produce incorrect outputs while remaining visually similar to the original inputs. The key steps of the PGD attack are as follows:\n",
        "\n",
        "1. **Initialization**: Start with an original input sample $x$ from the dataset.\n",
        "2. **Perturbation**: Add a small, carefully chosen perturbation ηη to the input $x$ such that the new input $x′=x+η$ misleads the model. The perturbation is bounded by a parameter $ϵ$ (epsilon), which controls the maximum allowed distortion.\n",
        "3. **Iterative Optimization**: Apply gradient ascent iteratively to maximize the model's loss with respect to the input $x′$. After each iteration, project $x′$ back into the $ϵ$-ball around $x$ to ensure the perturbation remains within the specified bounds.\n",
        "\n",
        "Mathematically, the PGD attack can be formulated as:\n",
        "$$xt+1′=clipx,ϵ(xt′+α⋅sign(∇xL(θ,xt′,y)))$$\n",
        "where $clipx,ϵ$ projects the perturbed input back into the $ϵ$-ball around $x, α$ is the step size, $∇xL(θ,xt′,y)$ is the gradient of the loss with respect to the input, and $L$ is the loss function.\n",
        "\n",
        "#### **Impact of Epsilon ($ε$)**\n",
        "\n",
        "The parameter ϵϵ plays a critical role in determining the effectiveness of the PGD attack. It defines the maximum amount of perturbation allowed for each input. The impact of ϵϵ can be summarized as follows:\n",
        "\n",
        "* Small $ϵ$: When ϵϵ is small, the perturbations are minimal, and the adversarial examples remain very close to the original inputs. This might lead to limited success in fooling the model, especially if the model is robust to small perturbations.\n",
        "* Large $ϵ$: As $ϵ$ increases, the perturbations become more noticeable, potentially making the adversarial examples more effective in causing misclassification. However, very large $ϵ$ values can result in perturbed inputs that no longer resemble the original data, which may be unrealistic in practical scenarios.\n",
        "\n",
        "The trade-off is to find an optimal $ϵ$ that balances the attack's success rate and the visual similarity of the adversarial examples to the original inputs.\n",
        "\n",
        "#### **Results and Observations**\n",
        "\n",
        "In the implementation on the CIFAR10 dataset, varying ϵϵ values demonstrated the following:\n",
        "\n",
        "* Low $ϵ$ (e.g., 0.1): The model's accuracy drops slightly, indicating that minor perturbations are not significantly affecting the model's predictions.\n",
        "* Moderate $ϵ$ (e.g., 0.3): The model's accuracy decreases more substantially, suggesting that the adversarial examples are effectively misleading the model.\n",
        "* High $ϵ$ (e.g., 0.5): The model's accuracy drops drastically, but the perturbations become more visible, potentially compromising the adversarial example's subtlety.\n",
        "\n",
        "#### **Conclusion**\n",
        "\n",
        "The PGD attack on CIFAR10 illustrates the vulnerability of neural networks to adversarial examples and highlights the importance of robust model training. The choice of ϵϵ is crucial, as it influences both the effectiveness of the attack and the perceptual similarity of the adversarial examples to the original inputs. By experimenting with different ϵϵ values, one can understand the trade-offs involved and work towards developing more resilient machine learning models."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "sec-ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}